{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from crewai import Agent, Task, Crew\n",
    "from crewai.tools import BaseTool\n",
    "from textwrap import dedent\n",
    "from typing import List, Dict, Any, Optional, Type, Union\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "OUTPUT_DIR = \"/home/irisowner/dev/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the LLM provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "def get_facilis_llm():\n",
    "    \"\"\"Returns the appropriate chat model based on AI_ENGINE selection.\"\"\"\n",
    "    from crewai import LLM\n",
    "    \n",
    "    ai_engine = os.getenv(\"AI_ENGINE\")\n",
    "    api_key = os.getenv(\"API_KEY\")\n",
    "    model_name = os.getenv(\"LLM_MODEL_NAME\")\n",
    "    \n",
    "\n",
    "    if ai_engine == \"openai\":\n",
    "        from openai import OpenAI\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        return LLM(model=model_name, temperature=0)  # Return just the model name\n",
    "\n",
    "    if ai_engine in [\"azureopenai\", \"azure_openai\"]:\n",
    "        azure_endpoint = os.getenv(\"AZURE_ENDPOINT\")\n",
    "        azure_deployment_name = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "        if not azure_endpoint or not azure_deployment_name:\n",
    "            raise ValueError(\"Azure OpenAI requires AZURE_ENDPOINT and AZURE_DEPLOYMENT_NAME in .env\")\n",
    "        os.environ[\"AZURE_OPENAI_API_KEY\"] = api_key\n",
    "        os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_endpoint\n",
    "        return azure_deployment_name  # Return the deployment name for Azure\n",
    "\n",
    "    if ai_engine in [\"anthropic\", \"claude\"]:\n",
    "        os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
    "        return LLM(model=model_name, temperature=0)\n",
    "\n",
    "    if ai_engine == \"gemini\":\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "        return LLM(model=model_name, temperature=0)\n",
    "    \n",
    "    if ai_engine == \"ollama\":\n",
    "        return LLM(model=model_name, temperature=0)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionData:\n",
    "    def __init__(self):\n",
    "        self.productions = {}  # Store productions and their endpoints\n",
    "        \n",
    "    def add_production(self, name: str, namespace: str):\n",
    "        if name not in self.productions:\n",
    "            self.productions[name] = {\n",
    "                'namespace': namespace,\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'endpoints': []\n",
    "            }\n",
    "            \n",
    "    def add_endpoint(self, production_name: str, endpoint_data: Dict):\n",
    "        if production_name in self.productions:\n",
    "            self.productions[production_name]['endpoints'].append(endpoint_data)\n",
    "            \n",
    "    def production_exists(self, name: str) -> bool:\n",
    "        return name in self.productions\n",
    "\n",
    "class OpenAPITransformer:\n",
    "    @staticmethod\n",
    "    def create_openapi_base(info: Dict) -> Dict:\n",
    "        return {\n",
    "            \"openapi\": \"3.0.0\",\n",
    "            \"info\": {\n",
    "                \"title\": info.get(\"production_name\", \"API Documentation\"),\n",
    "                \"version\": \"1.0.0\",\n",
    "                \"description\": f\"API documentation for {info.get('production_name')} in {info.get('namespace')}\"\n",
    "            },\n",
    "            \"paths\": {},\n",
    "            \"components\": {\n",
    "                \"schemas\": {},\n",
    "                \"securitySchemes\": {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def create_path_item(endpoint_spec: Dict) -> Dict:\n",
    "        method = endpoint_spec[\"HTTP_Method\"].lower()\n",
    "        path_item = {\n",
    "            method: {\n",
    "                \"summary\": endpoint_spec.get(\"description\", \"\"),\n",
    "                \"parameters\": [],\n",
    "                \"responses\": {\n",
    "                    \"200\": {\n",
    "                        \"description\": \"Successful response\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if endpoint_spec.get(\"params\"):\n",
    "            for param in endpoint_spec[\"params\"]:\n",
    "                path_item[method][\"parameters\"].append({\n",
    "                    \"name\": param,\n",
    "                    \"in\": \"query\",\n",
    "                    \"required\": False,\n",
    "                    \"schema\": {\"type\": \"string\"}\n",
    "                })\n",
    "\n",
    "        if method in [\"post\", \"put\", \"patch\"] and endpoint_spec.get(\"json_model\"):\n",
    "            path_item[method][\"requestBody\"] = {\n",
    "                \"required\": True,\n",
    "                \"content\": {\n",
    "                    \"application/json\": {\n",
    "                        \"schema\": endpoint_spec[\"json_model\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "        return path_item\n",
    "\n",
    "\n",
    "def extract_json_from_markdown(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts JSON content from markdown-formatted string.\n",
    "    Handles cases where JSON is wrapped in ```json or ``` code blocks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the CrewOutput to string if it isn't already\n",
    "        text = str(markdown_text)\n",
    "        \n",
    "        # If the text is already a valid JSON string, return it\n",
    "        try:\n",
    "            json.loads(text)\n",
    "            return text\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Try to extract JSON from markdown code blocks\n",
    "        if '```json' in text:\n",
    "            # Split by ```json and take the content after it\n",
    "            parts = text.split('```json')\n",
    "            if len(parts) > 1:\n",
    "                # Split by ``` to get the content between the code block\n",
    "                json_text = parts[1].split('```')[0]\n",
    "                return json_text.strip()\n",
    "        elif '```' in text:\n",
    "            # Split by ``` and take the content between code blocks\n",
    "            parts = text.split('```')\n",
    "            if len(parts) > 1:\n",
    "                potential_json = parts[1].strip()\n",
    "                try:\n",
    "                    json.loads(potential_json)\n",
    "                    return potential_json\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "\n",
    "        # If no code blocks, try to find JSON between curly braces\n",
    "        start = text.find('{')\n",
    "        end = text.rindex('}') + 1\n",
    "        \n",
    "        if start != -1 and end > start:\n",
    "            json_str = text[start:end]\n",
    "            # Validate that this is valid JSON\n",
    "            json.loads(json_str)\n",
    "            return json_str\n",
    "            \n",
    "        # If we couldn't find valid JSON, try to create a simple JSON object\n",
    "        # from the text itself\n",
    "        return json.dumps({\"response\": text})\n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": \"Failed to parse response\", \"raw_response\": text})\n",
    "\n",
    "class IRISClassWriter:\n",
    "    \"\"\"Tool for generating InterSystems IRIS interoperability class files\"\"\"\n",
    "    _instance = None\n",
    "    _initialized = False\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, output_dir=OUTPUT_DIR):\n",
    "        if not IRISClassWriter._initialized:\n",
    "            self.output_dir = os.path.abspath(output_dir)\n",
    "            self.generated_classes = {}\n",
    "            self._ensure_output_directory()\n",
    "            IRISClassWriter._initialized = True\n",
    "\n",
    "    def validate_classes(self, class_names: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate the generated IRIS classes\n",
    "        \n",
    "        Args:\n",
    "            class_names (Optional[List[str]]): Optional list of specific class names to validate\n",
    "            \n",
    "        Returns:\n",
    "            dict: Validation results for each class\n",
    "        \"\"\"\n",
    "        validation_results = {}\n",
    "        \n",
    "        classes_to_validate = (\n",
    "            class_names if class_names \n",
    "            else list(self.generated_classes.keys())\n",
    "        )\n",
    "        \n",
    "        for class_name in classes_to_validate:\n",
    "            if class_name not in self.generated_classes:\n",
    "                validation_results[class_name] = {\n",
    "                    \"valid\": False,\n",
    "                    \"issues\": [\"Class not found in generated classes\"]\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            class_content = self.generated_classes[class_name]\n",
    "            issues = []\n",
    "            \n",
    "            # Basic validation checks\n",
    "            if not \"Class \" in class_content:\n",
    "                issues.append(\"Missing Class declaration\")\n",
    "            \n",
    "            if not \"Extends \" in class_content:\n",
    "                issues.append(\"Missing Extends keyword\")\n",
    "            \n",
    "            # Add more validation checks as needed\n",
    "            \n",
    "            validation_results[class_name] = {\n",
    "                \"valid\": len(issues) == 0,\n",
    "                \"issues\": issues\n",
    "            }\n",
    "        \n",
    "        return validation_results\n",
    "\n",
    "    def _ensure_output_directory(self):\n",
    "        \"\"\"Ensures the output directory exists and is writable\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            # Test if directory is writable\n",
    "            test_file = os.path.join(self.output_dir, '.write_test')\n",
    "            try:\n",
    "                with open(test_file, 'w') as f:\n",
    "                    f.write('test')\n",
    "                os.remove(test_file)\n",
    "            except (IOError, OSError) as e:\n",
    "                raise PermissionError(f\"Output directory {self.output_dir} is not writable: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to create output directory {self.output_dir}: {str(e)}\")\n",
    "    \n",
    "    def write_production_class(self, production_name, components):\n",
    "        \"\"\"\n",
    "        Generate an IRIS Production class\n",
    "        \n",
    "        Args:\n",
    "            production_name (str): Name of the production\n",
    "            components (list): List of components to include in the production\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated class content\n",
    "        \"\"\"\n",
    "        class_name = self._sanitize_class_name(production_name)\n",
    "        \n",
    "        class_content = f\"\"\"Class {class_name} Extends Ens.Production\n",
    "        {{\n",
    "\n",
    "        XData ProductionDefinition\n",
    "        {{\n",
    "        <Production Name=\"{class_name}\" LogGeneralTraceEvents=\"false\">\n",
    "        \"\"\"\n",
    "            \n",
    "        # Add components\n",
    "        for component in components:\n",
    "            comp_type = component.get(\"type\", \"\")\n",
    "            comp_name = component.get(\"name\", \"\")\n",
    "            comp_class = component.get(\"class\", \"\")\n",
    "            \n",
    "            if comp_type and comp_name and comp_class:\n",
    "                class_content += f\"\"\"  <{comp_type} Name=\"{comp_name}\" Class=\"{comp_class}\">\n",
    "        </{comp_type}>\n",
    "        \"\"\"\n",
    "                \n",
    "                class_content += \"\"\"</Production>\n",
    "        }}\n",
    "\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        self.generated_classes[class_name] = class_content\n",
    "        return class_content\n",
    "    \n",
    "    def write_business_operation(self, operation_name, endpoint_info):\n",
    "        \"\"\"\n",
    "        Generate an IRIS Business Operation class\n",
    "        \n",
    "        Args:\n",
    "            operation_name (str): Name of the business operation\n",
    "            endpoint_info (dict): Information about the API endpoint\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated class content\n",
    "        \"\"\"\n",
    "        class_name = self._sanitize_class_name(f\"bo{operation_name}\")\n",
    "        \n",
    "        method = endpoint_info.get(\"method\", \"GET\")\n",
    "        path = endpoint_info.get(\"path\", \"/\")\n",
    "        \n",
    "        class_content = f\"\"\"Class {class_name} Extends Ens.BusinessOperation\n",
    "        {{\n",
    "\n",
    "        Parameter ADAPTER = \"EnsLib.HTTP.OutboundAdapter\";\n",
    "\n",
    "        Property Adapter As EnsLib.HTTP.OutboundAdapter;\n",
    "\n",
    "        Parameter INVOCATION = \"Queue\";\n",
    "\n",
    "        Method {operation_name}(pRequest As ms{operation_name}, Output pResponse As Ens.Response) As %Status\n",
    "        {{\n",
    "            Set tSC = $$$OK\n",
    "            Try {{\n",
    "                // Prepare HTTP request\n",
    "                Set tHttpRequest = ##class(%Net.HttpRequest).%New()\n",
    "                Set tHttpRequest.ContentType = \"application/json\"\n",
    "                \n",
    "                // Set request path and method\n",
    "                Set tPath = \"{path}\"\n",
    "                Set tMethod = \"{method}\"\n",
    "                \n",
    "                // Convert request message to JSON\n",
    "                // [Additional logic for request preparation]\n",
    "                \n",
    "                // Send the HTTP request\n",
    "                Set tSC = ..Adapter.SendFormDataArray(.tHttpResponse, tMethod, tPath, tHttpRequest)\n",
    "                \n",
    "                // Process response\n",
    "                If $$$ISOK(tSC) {{\n",
    "                    // Create response object\n",
    "                    Set pResponse = ##class(Ens.Response).%New()\n",
    "                    // Process HTTP response\n",
    "                }}\n",
    "            }}\n",
    "            Catch ex {{\n",
    "                Set tSC = ex.AsStatus()\n",
    "            }}\n",
    "            \n",
    "            Return tSC\n",
    "        }}\n",
    "\n",
    "        XData MessageMap\n",
    "        {{\n",
    "        <MapItems>\n",
    "        <MapItem MessageType=\"ms{operation_name}\">\n",
    "            <Method>{operation_name}</Method>\n",
    "        </MapItem>\n",
    "        </MapItems>\n",
    "        }}\n",
    "\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.generated_classes[class_name] = class_content\n",
    "        return class_content\n",
    "    \n",
    "    def write_message_class(self, message_name, schema_info):\n",
    "        \"\"\"\n",
    "        Generate an IRIS Message class\n",
    "        \n",
    "        Args:\n",
    "            message_name (str): Name of the message class\n",
    "            schema_info (dict): Information about the schema\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated class content\n",
    "        \"\"\"\n",
    "        class_name = self._sanitize_class_name(f\"ms{message_name}\")\n",
    "        \n",
    "        class_content = f\"\"\"Class {class_name} Extends Ens.Request\n",
    "        {{\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add properties based on schema\n",
    "        if isinstance(schema_info, dict) and \"properties\" in schema_info:\n",
    "            for prop_name, prop_info in schema_info[\"properties\"].items():\n",
    "                prop_type = self._map_schema_type_to_iris(prop_info.get(\"type\", \"string\"))\n",
    "                class_content += f\"Property {prop_name} As {prop_type};\\n\\n\"\n",
    "        \n",
    "        class_content += \"}\\n\"\n",
    "        \n",
    "        self.generated_classes[class_name] = class_content\n",
    "        return class_content\n",
    "    \n",
    "    def export_classes(self):\n",
    "        \"\"\"\n",
    "        Export all generated classes to .cls files\n",
    "        \n",
    "        Returns:\n",
    "            dict: Status of export operation\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if not self.generated_classes:\n",
    "            return {\"status\": \"warning\", \"message\": \"No classes to export\"}\n",
    "        \n",
    "        for class_name, class_content in self.generated_classes.items():\n",
    "            file_path = os.path.join(self.output_dir, f\"{class_name}.cls\")\n",
    "            \n",
    "            try:\n",
    "                # Ensure the directory exists (including package directories)\n",
    "                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "                \n",
    "                # Write the file with proper encoding\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(class_content)\n",
    "                \n",
    "                results[class_name] = {\n",
    "                    \"status\": \"success\",\n",
    "                    \"path\": file_path,\n",
    "                    \"size\": os.path.getsize(file_path)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results[class_name] = {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e),\n",
    "                    \"path\": file_path\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def validate_classes(self):\n",
    "        \"\"\"\n",
    "        Validate the generated IRIS classes for syntax and structural correctness\n",
    "        \n",
    "        Returns:\n",
    "            dict: Validation results\n",
    "        \"\"\"\n",
    "        validation_results = {}\n",
    "        \n",
    "        for class_name, class_content in self.generated_classes.items():\n",
    "            issues = []\n",
    "            \n",
    "            # Basic validation checks\n",
    "            if not \"Class \" in class_content:\n",
    "                issues.append(\"Missing Class declaration\")\n",
    "            \n",
    "            if not \"Extends \" in class_content:\n",
    "                issues.append(\"Missing Extends keyword\")\n",
    "            \n",
    "            # Add more validation as needed\n",
    "            \n",
    "            validation_results[class_name] = {\n",
    "                \"valid\": len(issues) == 0,\n",
    "                \"issues\": issues\n",
    "            }\n",
    "        \n",
    "        return validation_results\n",
    "\n",
    "    def _sanitize_class_name(self, name):\n",
    "        \"\"\"Sanitize a name to be valid as an IRIS class name\"\"\"\n",
    "        if not name:\n",
    "            raise ValueError(\"Class name cannot be empty\")\n",
    "        if len(name) > 255:  # Example max length\n",
    "            raise ValueError(\"Class name too long\")\n",
    "\n",
    "        # Replace non-alphanumeric characters with underscores\n",
    "        name = re.sub(r'[^a-zA-Z0-9_]', '_', name)\n",
    "        \n",
    "        # Ensure first character is a letter\n",
    "        if not name[0].isalpha():\n",
    "            name = \"X\" + name\n",
    "        \n",
    "        return name\n",
    "    \n",
    "    def _map_schema_type_to_iris(self, schema_type):\n",
    "        \"\"\"Map OpenAPI schema type to IRIS type\"\"\"\n",
    "        type_mapping = {\n",
    "            \"string\": \"%String\",\n",
    "            \"integer\": \"%Integer\",\n",
    "            \"number\": \"%Float\",\n",
    "            \"boolean\": \"%Boolean\",\n",
    "            \"array\": \"%Library.ListOfDataTypes\",\n",
    "            \"object\": \"%DynamicObject\"\n",
    "        }\n",
    "        \n",
    "        return type_mapping.get(schema_type, \"%String\")\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"\n",
    "    Sanitize a string to be used as a filename\n",
    "    \n",
    "    Args:\n",
    "        name (str): The input string\n",
    "        \n",
    "    Returns:\n",
    "        str: A sanitized filename\n",
    "    \"\"\"\n",
    "    # Remove invalid characters for filenames\n",
    "    invalid_chars = '<>:\"/\\\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        name = name.replace(char, '_')\n",
    "    \n",
    "    # Ensure it doesn't start with a space or period\n",
    "    if name.startswith(' ') or name.startswith('.'):\n",
    "        name = 'x' + name\n",
    "    \n",
    "    return name\n",
    "\n",
    "class OpenAPIParser:\n",
    "    \"\"\"Tool for parsing and analyzing OpenAPI v3 specifications\"\"\"\n",
    "\n",
    "    def analyze(self, openapi_spec):\n",
    "        \"\"\"\n",
    "        Analyzes an OpenAPI specification and returns structured information\n",
    "        \n",
    "        Args:\n",
    "            openapi_spec (Union[dict, str]): The OpenAPI specification as a Python dictionary or JSON string\n",
    "            \n",
    "        Returns:\n",
    "            dict: Structured analysis of the OpenAPI specification\n",
    "        \"\"\"\n",
    "        if isinstance(openapi_spec, str):\n",
    "            try:\n",
    "                openapi_spec = json.loads(openapi_spec)\n",
    "            except json.JSONDecodeError as e:\n",
    "                return {\"error\": f\"Invalid JSON string provided: {str(e)}\"}\n",
    "        elif not isinstance(openapi_spec, dict):\n",
    "            return {\"error\": \"Input must be either a JSON string or a dictionary\"}\n",
    "\n",
    "        try:\n",
    "            result = {\n",
    "                \"info\": self._extract_info(openapi_spec),\n",
    "                \"endpoints\": self._extract_endpoints(openapi_spec),\n",
    "                \"schemas\": self._extract_schemas(openapi_spec)\n",
    "            }\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Analysis failed: {str(e)}\"}\n",
    "    \n",
    "    def _extract_info(self, spec):\n",
    "        \"\"\"Extract basic information from the API spec\"\"\"\n",
    "        info = spec.get(\"info\", {})\n",
    "        return {\n",
    "            \"title\": info.get(\"title\", \"Unknown API\"),\n",
    "            \"version\": info.get(\"version\", \"1.0.0\"),\n",
    "            \"description\": info.get(\"description\", \"\")\n",
    "        }\n",
    "    \n",
    "    def _extract_endpoints(self, spec):\n",
    "        \"\"\"Extract endpoint details from the paths section\"\"\"\n",
    "        paths = spec.get(\"paths\", {})\n",
    "        endpoints = []\n",
    "        \n",
    "        for path, path_item in paths.items():\n",
    "            for method, operation in path_item.items():\n",
    "                if method in [\"get\", \"post\", \"put\", \"delete\", \"patch\"]:\n",
    "                    endpoint = {\n",
    "                        \"path\": path,\n",
    "                        \"method\": method.upper(),\n",
    "                        \"operationId\": operation.get(\"operationId\", f\"{method}_{path.replace('/', '_')}\"),\n",
    "                        \"summary\": operation.get(\"summary\", \"\"),\n",
    "                        \"description\": operation.get(\"description\", \"\"),\n",
    "                        \"parameters\": operation.get(\"parameters\", []),\n",
    "                        \"requestBody\": operation.get(\"requestBody\", None),\n",
    "                        \"responses\": operation.get(\"responses\", {})\n",
    "                    }\n",
    "                    endpoints.append(endpoint)\n",
    "        \n",
    "        return endpoints\n",
    "    \n",
    "    def _extract_schemas(self, spec):\n",
    "        \"\"\"Extract schema definitions\"\"\"\n",
    "        components = spec.get(\"components\", {})\n",
    "        schemas = components.get(\"schemas\", {})\n",
    "        \n",
    "        return {name: details for name, details in schemas.items()}\n",
    "\n",
    "\n",
    "class AnalyzeOpenAPIToolInput(BaseModel):\n",
    "    openapi_spec: Union[str, Dict[str, Any]] = Field(\n",
    "        description=\"OpenAPI specification as JSON string or dictionary\"\n",
    "    )\n",
    "\n",
    "class AnalyzeOpenAPITool(BaseTool):\n",
    "    name: str = \"analyze_openapi\"\n",
    "    description: str = \"Analyzes an OpenAPI specification and returns structured information\"\n",
    "    input_schema: Type[BaseModel] = AnalyzeOpenAPIToolInput\n",
    "\n",
    "    def _run(self, openapi_spec: Union[str, Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Analyzes an OpenAPI specification and returns structured information\n",
    "        \n",
    "        Args:\n",
    "            openapi_spec: The OpenAPI specification as a JSON string or dictionary\n",
    "            \n",
    "        Returns:\n",
    "            str: JSON string containing structured analysis\n",
    "        \"\"\"\n",
    "        parser = OpenAPIParser()\n",
    "        \n",
    "        try:\n",
    "            # If input is string, try to parse it as JSON\n",
    "            if isinstance(openapi_spec, str):\n",
    "                try:\n",
    "                    spec_dict = json.loads(openapi_spec)\n",
    "                except json.JSONDecodeError:\n",
    "                    return json.dumps({\"error\": \"Invalid JSON string provided\"})\n",
    "            else:\n",
    "                # If it's already a dictionary, use it directly\n",
    "                spec_dict = openapi_spec\n",
    "            \n",
    "            result = parser.analyze(spec_dict)\n",
    "            return json.dumps(result, indent=2)\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Analysis failed: {str(e)}\"})\n",
    "\n",
    "class GenerateProductionClassTool(BaseTool):\n",
    "    name: str = \"generate_production_class\"\n",
    "    description: str = \"Generate an IRIS Production class\"\n",
    "\n",
    "    def _run(self, production_name: str, components: str) -> str:\n",
    "        writer = IRISClassWriter()\n",
    "        try:\n",
    "            components_list = json.loads(components)\n",
    "            return writer.write_production_class(production_name, components_list)\n",
    "        except json.JSONDecodeError:\n",
    "            return \"Error: Invalid JSON format for components\"\n",
    "\n",
    "class CollectGeneratedFilesToolInput(BaseModel):\n",
    "    directory: str = Field(\n",
    "        description=\"Directory containing the generated .cls files\"\n",
    "    )\n",
    "\n",
    "class CollectGeneratedFilesTool(BaseTool):\n",
    "    name: str = \"collect_generated_files\"\n",
    "    description: str = \"Collects all generated .cls files and returns them as a JSON collection\"\n",
    "    input_schema: Type[BaseModel] = CollectGeneratedFilesToolInput\n",
    "\n",
    "    def _run(self, directory: str) -> str:\n",
    "        \"\"\"\n",
    "        Collects all generated .cls files and returns them as a JSON collection\n",
    "        \n",
    "        Args:\n",
    "            directory (str): Directory containing the generated files\n",
    "            \n",
    "        Returns:\n",
    "            str: JSON string containing file contents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            files_collection = {}\n",
    "            for root, _, files in os.walk(directory):\n",
    "                for file in files:\n",
    "                    if file.endswith('.cls'):\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            files_collection[file] = {\n",
    "                                'path': os.path.relpath(file_path, directory),\n",
    "                                'content': f.read()\n",
    "                            }\n",
    "            return json.dumps(files_collection, indent=2)\n",
    "        except Exception as e:\n",
    "            return json.dumps({\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "class GenerateProductionClassToolInput(BaseModel):\n",
    "    production_name: str = Field(description=\"Name of the production\")\n",
    "    components: Union[str, List[Dict[str, Any]]] = Field(description=\"Components as JSON string or list of dictionaries\")\n",
    "\n",
    "class GenerateBusinessServiceToolInput(BaseModel):\n",
    "    service_name: str = Field(description=\"Name of the business service\")\n",
    "    endpoint_info: Union[str, Dict[str, Any]] = Field(description=\"Endpoint information as JSON string or dictionary\")\n",
    "\n",
    "class GenerateBusinessOperationToolInput(BaseModel):\n",
    "    operation_name: str = Field(description=\"Name of the business operation\")\n",
    "    endpoint_info: Union[str, Dict[str, Any]] = Field(description=\"Endpoint information as JSON string or dictionary\")\n",
    "\n",
    "class GenerateMessageClassToolInput(BaseModel):\n",
    "    message_name: str = Field(description=\"Name of the message class\")\n",
    "    schema_info: Union[str, Dict[str, Any]] = Field(description=\"Schema information as JSON string or dictionary\")\n",
    "\n",
    "class GenerateBusinessServiceTool(BaseTool):\n",
    "    name: str = \"generate_business_service\"\n",
    "    description: str = \"Generate an IRIS Business Service class\"\n",
    "    input_schema: Type[BaseModel] = GenerateBusinessServiceToolInput\n",
    "\n",
    "    def _run(self, service_name: str, endpoint_info: Union[str, Dict[str, Any]]) -> str:\n",
    "        writer = IRISClassWriter()\n",
    "        try:\n",
    "            if isinstance(endpoint_info, str):\n",
    "                try:\n",
    "                    endpoint_dict = json.loads(endpoint_info)\n",
    "                except json.JSONDecodeError:\n",
    "                    return \"Error: Invalid JSON format for endpoint info\"\n",
    "            else:\n",
    "                endpoint_dict = endpoint_info\n",
    "\n",
    "            class_content = writer.write_business_service(service_name, endpoint_dict)\n",
    "            # Store the generated class\n",
    "            writer.generated_classes[f\"BS.{service_name}\"] = class_content\n",
    "            return class_content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating business service: {str(e)}\"\n",
    "\n",
    "class GenerateBusinessOperationTool(BaseTool):\n",
    "    name: str = \"generate_business_operation\"\n",
    "    description: str = \"Generate an IRIS Business Operation class\"\n",
    "    input_schema: Type[BaseModel] = GenerateBusinessOperationToolInput\n",
    "\n",
    "    def _run(self, operation_name: str, endpoint_info: Union[str, Dict[str, Any]]) -> str:\n",
    "        writer = IRISClassWriter()\n",
    "        try:\n",
    "            if isinstance(endpoint_info, str):\n",
    "                try:\n",
    "                    endpoint_dict = json.loads(endpoint_info)\n",
    "                except json.JSONDecodeError:\n",
    "                    return \"Error: Invalid JSON format for endpoint info\"\n",
    "            else:\n",
    "                endpoint_dict = endpoint_info\n",
    "\n",
    "            class_content = writer.write_business_operation(operation_name, endpoint_dict)\n",
    "            # Store the generated class\n",
    "            writer.generated_classes[f\"BO.{operation_name}\"] = class_content\n",
    "            return class_content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating business operation: {str(e)}\"\n",
    "\n",
    "class GenerateMessageClassTool(BaseTool):\n",
    "    name: str = \"generate_message_class\"\n",
    "    description: str = \"Generate an IRIS Message class\"\n",
    "    input_schema: Type[BaseModel] = GenerateMessageClassToolInput\n",
    "\n",
    "    def _run(self, message_name: str, schema_info: Union[str, Dict[str, Any]]) -> str:\n",
    "        writer = IRISClassWriter()\n",
    "        try:\n",
    "            if isinstance(schema_info, str):\n",
    "                try:\n",
    "                    schema_dict = json.loads(schema_info)\n",
    "                except json.JSONDecodeError:\n",
    "                    return \"Error: Invalid JSON format for schema info\"\n",
    "            else:\n",
    "                schema_dict = schema_info\n",
    "\n",
    "            class_content = writer.write_message_class(message_name, schema_dict)\n",
    "            # Store the generated class\n",
    "            writer.generated_classes[f\"MSG.{message_name}\"] = class_content\n",
    "            return class_content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating message class: {str(e)}\"\n",
    "\n",
    "class GenerateProductionClassTool(BaseTool):\n",
    "    name: str = \"generate_production_class\"\n",
    "    description: str = \"Generate an IRIS Production class\"\n",
    "    input_schema: Type[BaseModel] = GenerateProductionClassToolInput\n",
    "\n",
    "    def _run(self, production_name: str, components: Union[str, List[Dict[str, Any]]]) -> str:\n",
    "        writer = IRISClassWriter()\n",
    "        try:\n",
    "            if isinstance(components, str):\n",
    "                try:\n",
    "                    components_list = json.loads(components)\n",
    "                except json.JSONDecodeError:\n",
    "                    return \"Error: Invalid JSON format for components\"\n",
    "            else:\n",
    "                components_list = components\n",
    "\n",
    "            class_content = writer.write_production_class(production_name, components_list)\n",
    "            # Store the generated class\n",
    "            writer.generated_classes[f\"Production.{production_name}\"] = class_content\n",
    "            return class_content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating production class: {str(e)}\"\n",
    "\n",
    "class ExportIRISClassesToolInput(BaseModel):\n",
    "    output_dir: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Optional output directory path. If not provided, will use default directory\"\n",
    "    )\n",
    "\n",
    "class ExportIRISClassesTool(BaseTool):\n",
    "    name: str = \"export_iris_classes\"\n",
    "    description: str = \"Export all generated classes to .cls files\"\n",
    "    input_schema: Type[BaseModel] = ExportIRISClassesToolInput\n",
    "\n",
    "    def _run(self, output_dir: Optional[str] = None) -> str:\n",
    "        writer = IRISClassWriter()\n",
    "        try:\n",
    "            if not writer.generated_classes:\n",
    "                return json.dumps({\n",
    "                    \"status\": \"warning\",\n",
    "                    \"message\": \"No classes to export\",\n",
    "                    \"details\": \"The generated_classes dictionary is empty. Make sure classes were generated successfully before exporting.\"\n",
    "                })\n",
    "\n",
    "            if output_dir:\n",
    "                writer.output_dir = os.path.abspath(output_dir)\n",
    "                writer._ensure_output_directory()\n",
    "\n",
    "            results = writer.export_classes()\n",
    "            return json.dumps({\n",
    "                \"status\": \"success\",\n",
    "                \"message\": f\"Exported {len(writer.generated_classes)} classes\",\n",
    "                \"details\": results\n",
    "            }, indent=2)\n",
    "        except Exception as e:\n",
    "            return json.dumps({\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "class ValidateIRISClassesToolInput(BaseModel):\n",
    "    class_names: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=\"Optional list of specific class names to validate. If not provided, validates all classes\"\n",
    "    )\n",
    "\n",
    "class ValidateIRISClassesTool(BaseTool):\n",
    "    name: str = \"validate_iris_classes\"\n",
    "    description: str = \"Validate the generated IRIS classes\"\n",
    "    input_schema: Type[BaseModel] = ValidateIRISClassesToolInput\n",
    "\n",
    "    def _run(self, class_names: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Validate the generated IRIS classes\n",
    "        \n",
    "        Args:\n",
    "            class_names (Optional[List[str]]): Optional list of specific class names to validate\n",
    "            \n",
    "        Returns:\n",
    "            str: JSON string containing validation results\n",
    "        \"\"\"\n",
    "        writer = IRISClassWriter()\n",
    "        try:\n",
    "            results = writer.validate_classes(class_names)\n",
    "            return json.dumps(results, indent=2)\n",
    "        except Exception as e:\n",
    "            return json.dumps({\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "# Create tool instances\n",
    "analyze_openapi_tool = AnalyzeOpenAPITool()\n",
    "generate_production_class_tool = GenerateProductionClassTool()\n",
    "generate_business_service_tool = GenerateBusinessServiceTool()\n",
    "generate_business_operation_tool = GenerateBusinessOperationTool()\n",
    "generate_message_class_tool = GenerateMessageClassTool()\n",
    "export_iris_classes_tool = ExportIRISClassesTool()\n",
    "validate_iris_classes_tool = ValidateIRISClassesTool()\n",
    "collect_generated_files_tool = CollectGeneratedFilesTool()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class APIAgents:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def create_production_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role='Production Manager',\n",
    "            goal='Manage production environments and namespaces',\n",
    "            backstory=dedent(\"\"\"\n",
    "                You are responsible for managing production environments and their namespaces.\n",
    "                You interact with users to gather production details and validate their existence.\n",
    "            \"\"\"),\n",
    "            allow_delegation=False,\n",
    "            llm=self.llm,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def create_interaction_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role='User Interaction Specialist',\n",
    "            goal='Interact with users to obtain missing API specification fields',\n",
    "            backstory=dedent(\"\"\"\n",
    "                You are a specialist in user interaction, responsible for identifying\n",
    "                and requesting missing information in API specifications.\n",
    "            \"\"\"),\n",
    "            allow_delegation=False,\n",
    "            llm=self.llm,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def create_validation_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role='API Validator',\n",
    "            goal='Validate API specifications for correctness and consistency',\n",
    "            backstory=dedent(\"\"\"\n",
    "                You are an expert in API validation, ensuring all specifications\n",
    "                meet the required standards and format.\n",
    "            \"\"\"),\n",
    "            allow_delegation=False,\n",
    "            llm=self.llm,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def create_extraction_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role='API Specification Extractor',\n",
    "            goal='Extract API specifications from natural language descriptions',\n",
    "            backstory=dedent(\"\"\"\n",
    "                You are specialized in interpreting natural language descriptions\n",
    "                and extracting structured API specifications.\n",
    "            \"\"\"),\n",
    "            allow_delegation=True,\n",
    "            llm=self.llm,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def create_transformation_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role='OpenAPI Transformation Specialist',\n",
    "            goal='Convert API specifications into OpenAPI documentation',\n",
    "            backstory=dedent(\"\"\"\n",
    "                You are an expert in OpenAPI specifications and documentation.\n",
    "                Your role is to transform validated API details into accurate\n",
    "                and comprehensive OpenAPI 3.0 documentation.\n",
    "            \"\"\"),\n",
    "            allow_delegation=False,\n",
    "            llm=self.llm,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def create_reviewer_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role='OpenAPI Documentation Reviewer',\n",
    "            goal='Ensure OpenAPI documentation compliance and quality',\n",
    "            backstory=dedent(\"\"\"\n",
    "                You are the final authority on OpenAPI documentation quality and compliance.\n",
    "                With extensive experience in OpenAPI 3.0 specifications, you meticulously\n",
    "                review documentation for accuracy, completeness, and adherence to standards.\n",
    "            \"\"\"),\n",
    "            allow_delegation=True,\n",
    "            llm=self.llm,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def create_analyzer_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"OpenAPI Specification Analyzer\",\n",
    "            goal=\"Thoroughly analyze OpenAPI specifications and plan IRIS Interoperability components\",\n",
    "            backstory=\"\"\"You are an expert in both OpenAPI specifications and InterSystems IRIS Interoperability. \n",
    "            Your job is to analyze OpenAPI documents and create a detailed plan for how they should be \n",
    "            implemented as IRIS Interoperability components.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            tools=[analyze_openapi_tool],\n",
    "            llm=self.llm\n",
    "        )\n",
    "\n",
    "    def create_bs_generator_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"IRIS Production and Business Service Generator\",\n",
    "            goal=\"Generate properly formatted IRIS Production and Business Service classes from OpenAPI specifications\",\n",
    "            backstory=\"\"\"You are an experienced InterSystems IRIS developer specializing in Interoperability Productions.\n",
    "            Your expertise is in creating Business Services and Productions that can receive and process incoming requests based on\n",
    "            API specifications.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=True,\n",
    "            tools=[generate_production_class_tool, generate_business_service_tool],\n",
    "            llm=self.llm\n",
    "        )\n",
    "\n",
    "    def create_bo_generator_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"IRIS Business Operation Generator\",\n",
    "            goal=\"Generate properly formatted IRIS Business Operation classes from OpenAPI specifications\",\n",
    "            backstory=\"\"\"You are an experienced InterSystems IRIS developer specializing in Interoperability Productions.\n",
    "            Your expertise is in creating Business Operations that can send requests to external systems\n",
    "            based on API specifications.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=True,\n",
    "            tools=[generate_business_operation_tool, generate_message_class_tool],\n",
    "            llm=self.llm\n",
    "        )\n",
    "\n",
    "    def create_exporter_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"IRIS Class Exporter\",\n",
    "            goal=\"Export and validate IRIS class definitions to proper .cls files\",\n",
    "            backstory=\"\"\"You are an InterSystems IRIS deployment specialist. Your job is to ensure \n",
    "            that generated IRIS class definitions are properly exported as valid .cls files that \n",
    "            can be directly imported into an IRIS environment.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            tools=[export_iris_classes_tool, validate_iris_classes_tool],\n",
    "            llm=self.llm\n",
    "        )\n",
    "        \n",
    "    def create_collector_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"IRIS Class Collector\",\n",
    "            goal=\"Collect all generated IRIS class files into a JSON collection\",\n",
    "            backstory=\"\"\"You are a file system specialist responsible for gathering and \n",
    "            organizing generated IRIS class files into a structured collection.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            tools=[CollectGeneratedFilesTool()],\n",
    "            llm=self.llm\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class APISpecificationCrew:\n",
    "    def __init__(self, llm, production_data: ProductionData):\n",
    "        self.current_agent = None \n",
    "        api_agents = APIAgents(llm)\n",
    "        self.openapi_parser = OpenAPIParser()\n",
    "        self.iris_class_writer = IRISClassWriter(output_dir=OUTPUT_DIR)\n",
    "        self.production_agent = api_agents.create_production_agent()\n",
    "        self.interaction_agent = api_agents.create_interaction_agent()\n",
    "        self.validation_agent = api_agents.create_validation_agent()\n",
    "        self.extraction_agent = api_agents.create_extraction_agent()\n",
    "        self.transformation_agent = api_agents.create_transformation_agent()\n",
    "        self.reviewer_agent = api_agents.create_reviewer_agent()\n",
    "        self.production_data = production_data\n",
    "        self.analyzer_agent = api_agents.create_analyzer_agent()\n",
    "        self.bs_generator_agent = api_agents.create_bs_generator_agent()\n",
    "        self.bo_generator_agent = api_agents.create_bo_generator_agent()\n",
    "        self.exporter_agent = api_agents.create_exporter_agent()\n",
    "        self.collector_agent = api_agents.create_collector_agent()\n",
    "\n",
    "    def get_production_details(self) -> Task:\n",
    "        return Task(\n",
    "            description=dedent(\"\"\"\n",
    "                Interact with the user to obtain:\n",
    "                1. Production name\n",
    "                2. Namespace\n",
    "                \n",
    "                Check if the production exists in the system.\n",
    "                If it doesn't exist, confirm if a new production should be created.\n",
    "                \n",
    "                Return results in JSON format:\n",
    "                {\n",
    "                    \"production_name\": string,\n",
    "                    \"namespace\": string,\n",
    "                    \"exists\": boolean,\n",
    "                    \"create_new\": boolean\n",
    "                }\n",
    "            \"\"\"),\n",
    "            expected_output=\"\"\"A JSON object containing production details including name, namespace, existence status, and creation flag\"\"\",\n",
    "            agent=self.production_agent\n",
    "        )\n",
    "\n",
    "    def handle_missing_fields(self, missing_fields: List[str], endpoint_info: Dict) -> Task:\n",
    "        # TODO: Implement the callback logic here\n",
    "        return Task(\n",
    "            description= dedent(f\"\"\"\n",
    "            The following fields are missing for endpoint {endpoint_info.get('endpoint', 'unknown')}:\n",
    "            {', '.join(missing_fields)}\n",
    "            \n",
    "            Current endpoint info: {json.dumps(endpoint_info, indent=2)}\n",
    "            \n",
    "            For each missing field:\n",
    "            1. If HTTP_Method: Must be one of GET, POST, PUT, DELETE, PATCH\n",
    "            2. If json_model: Required for POST/PUT/PATCH methods\n",
    "            3. If params: List of parameter names\n",
    "            4. If production_name: Name of the production environment\n",
    "            5. If namespace: Namespace for the production\n",
    "            \n",
    "            Return the collected information in JSON format.\n",
    "        \"\"\"),\n",
    "            expected_output=\"A JSON object containing the updated field values\",\n",
    "            agent=self.interaction_agent\n",
    "        )\n",
    "        \n",
    "\n",
    "    def validate_api_spec(self, extracted_data: Dict) -> Task:\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                Validate the following API specification:\n",
    "                {json.dumps(extracted_data, indent=2)}\n",
    "                \n",
    "                Check for:\n",
    "                1. Valid host format\n",
    "                2. Endpoint starts with '/'\n",
    "                3. Valid HTTP method (GET, POST, PUT, DELETE, PATCH)\n",
    "                4. Valid port number (if provided)\n",
    "                5. JSON model presence for POST/PUT/PATCH/DELETE methods\n",
    "                \n",
    "                Return validation results in JSON format.\n",
    "            \"\"\"),\n",
    "            expected_output=\"\"\"A JSON object containing validation results with any errors or confirmation of validity\"\"\",\n",
    "            agent=self.validation_agent\n",
    "        )\n",
    "\n",
    "    def extract_api_specs(self, descriptions: List[str]) -> Task:\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                Extract API specifications from the following descriptions:\n",
    "                {json.dumps(descriptions, indent=2)}\n",
    "                \n",
    "                For each description, extract:\n",
    "                - host (required)\n",
    "                - endpoint (required)\n",
    "                - HTTP_Method (required)\n",
    "                - params (optional)\n",
    "                - port (if available)\n",
    "                - json_model (for POST/PUT/PATCH/DELETE)\n",
    "                - authentication (if applicable)\n",
    "                \n",
    "                Mark any missing required fields as 'missing'.\n",
    "                Return results in JSON format as an array of specifications.\n",
    "            \"\"\"),\n",
    "            expected_output=\"\"\"A JSON array containing extracted API specifications with all required and optional fields\"\"\",\n",
    "            agent=self.extraction_agent\n",
    "        )\n",
    "\n",
    "    def transform_to_openapi(self, validated_endpoints: List[Dict], production_info: Dict) -> Task:\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                Transform the following validated API specifications into OpenAPI 3.0 documentation:\n",
    "                \n",
    "                Production Information:\n",
    "                {json.dumps(production_info, indent=2)}\n",
    "                \n",
    "                Validated Endpoints:\n",
    "                {json.dumps(validated_endpoints, indent=2)}\n",
    "                \n",
    "                Requirements:\n",
    "                1. Generate complete OpenAPI 3.0 specification\n",
    "                2. Include proper request/response schemas\n",
    "                3. Document all parameters and request bodies\n",
    "                4. Include authentication if specified\n",
    "                5. Ensure proper path formatting\n",
    "                \n",
    "                Return the OpenAPI specification in both JSON and YAML formats.\n",
    "            \"\"\"),\n",
    "            expected_output=\"\"\"A JSON object containing the complete OpenAPI 3.0 specification with all endpoints and schemas\"\"\",\n",
    "            agent=self.transformation_agent\n",
    "        )\n",
    "\n",
    "    def review_openapi_spec(self, openapi_spec: Dict) -> Task:\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                Review the following OpenAPI specification for compliance and quality:\n",
    "                \n",
    "                {json.dumps(openapi_spec, indent=2)}\n",
    "                \n",
    "                Review Checklist:\n",
    "                1. OpenAPI 3.0 Compliance\n",
    "                - Verify correct version specification\n",
    "                - Check required root elements\n",
    "                - Validate schema structure\n",
    "                \n",
    "                2. Completeness\n",
    "                - All endpoints properly documented\n",
    "                - Parameters fully specified\n",
    "                - Request/response schemas defined\n",
    "                - Security schemes properly configured\n",
    "                \n",
    "                3. Quality Checks\n",
    "                - Consistent naming conventions\n",
    "                - Clear descriptions\n",
    "                - Proper use of data types\n",
    "                - Meaningful response codes\n",
    "                \n",
    "                4. Best Practices\n",
    "                - Proper tag usage\n",
    "                - Consistent parameter naming\n",
    "                - Appropriate security definitions\n",
    "                \n",
    "                You must return a JSON object with the following structure:\n",
    "                {{\n",
    "                    \"is_valid\": boolean,\n",
    "                    \"approved_spec\": object (the reviewed and possibly corrected OpenAPI spec),\n",
    "                    \"issues\": [array of strings describing any issues found],\n",
    "                    \"recommendations\": [array of improvement suggestions]\n",
    "                }}\n",
    "            \"\"\"),\n",
    "            expected_output=\"\"\"A JSON object containing: is_valid (boolean), approved_spec (object), issues (array), and recommendations (array)\"\"\",\n",
    "            agent=self.reviewer_agent\n",
    "        )\n",
    "\n",
    "\n",
    "    def analysis_task(self, openapi_spec: Dict) -> Task:\n",
    "        return Task(\n",
    "            description=\"\"\"Analyze the OpenAPI specification and plan the necessary IRIS Interoperability components. \n",
    "            Include a list of all components that should be in the Production class.\"\"\",\n",
    "            agent=self.analyzer,\n",
    "            expected_output=\"A detailed analysis of OpenAPI spec and plan for IRIS components, including Production components list\",\n",
    "            input={\n",
    "                \"openapi_spec\": openapi_spec,\n",
    "                \"production_name\": \"${production_name}\" \n",
    "            }\n",
    "        )\n",
    "\n",
    "    def bs_generation_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Generate Business Service classes based on the OpenAPI endpoints\",\n",
    "            agent=self.bs_generator,\n",
    "            expected_output=\"IRIS Business Service class definitions\",\n",
    "            context=[self.analysis_task]\n",
    "        )\n",
    "\n",
    "    def bo_generation_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Generate Business Operation classes based on the OpenAPI endpoints\",\n",
    "            agent=self.bo_generator,\n",
    "            expected_output=\"IRIS Business Operation class definitions\",\n",
    "            context=[self.analysis_task]\n",
    "        )\n",
    "\n",
    "    def export_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Export all generated IRIS classes as valid .cls files\",\n",
    "            agent=self.exporter,\n",
    "            expected_output=\"Valid IRIS .cls files saved to output directory\",\n",
    "            context=[self.bs_generation_task, self.bo_generation_task],\n",
    "            input={\n",
    "                \"output_dir\": OUTPUT_DIR\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def validate_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Validate all generated IRIS classes\",\n",
    "            agent=self.exporter,\n",
    "            expected_output=\"Validation results for all generated classes\",\n",
    "            context=[self.export_task],\n",
    "            input={\n",
    "                \"class_names\": None  # Optional, will validate all classes if not specified\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def production_generation_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Generate the Production class that includes all generated components\",\n",
    "            agent=self.bs_generator,  # We can use the bs_generator since it has the generate_production_class_tool\n",
    "            expected_output=\"IRIS Production class definition\",\n",
    "            context=[self.bs_generation_task, self.bo_generation_task],  # This ensures it runs after BS and BO generation\n",
    "        )\n",
    "\n",
    "    def production_generation_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Generate the Production class that includes all generated components\",\n",
    "            agent=self.bs_generator,  # We can use the bs_generator since it has the generate_production_class_tool\n",
    "            input={\n",
    "                \"production_name\": \"${production_name}\"  # Add production name input\n",
    "            },\n",
    "            expected_output=\"IRIS Production class definition\",\n",
    "            context=[self.bs_generation_task, self.bo_generation_task],  # This ensures it runs after BS and BO generation\n",
    "        )\n",
    "\n",
    "    def collection_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"Collect all generated IRIS class files into a JSON collection\",\n",
    "            agent=self.collector,\n",
    "            expected_output=\"JSON collection of all generated .cls files\",\n",
    "            context=[self.export_task, self.validate_task],\n",
    "            input={\n",
    "                \"directory\": OUTPUT_DIR\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process API Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data = ProductionData()\n",
    "\n",
    "try:\n",
    "    llm = get_facilis_llm()\n",
    "    if llm is None:\n",
    "        raise ValueError(\"Invalid AI_ENGINE selection\")\n",
    "\n",
    "    crew = APISpecificationCrew(llm, production_data)\n",
    "\n",
    "    # Process endpoints\n",
    "    endpoints = user_input.split('\\n')\n",
    "    \n",
    "    api_crew = Crew(\n",
    "        agents=[\n",
    "            crew.production_agent,\n",
    "            crew.extraction_agent,\n",
    "            crew.validation_agent,\n",
    "            crew.interaction_agent,\n",
    "            crew.transformation_agent,\n",
    "            crew.reviewer_agent,\n",
    "            crew.analyzer_agent, \n",
    "            crew.bs_generator_agent, \n",
    "            crew.bo_generator_agent, \n",
    "            crew.exporter_agent, \n",
    "            crew.collector_agent\n",
    "        ],\n",
    "        tasks=[crew.get_production_details()],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    production_result = api_crew.kickoff()\n",
    "    production_info = json.loads(extract_json_from_markdown(production_result))\n",
    "\n",
    "    if not isinstance(production_info, dict):\n",
    "        raise ValueError(\"Invalid production info format\")\n",
    "\n",
    "    if not production_info.get('production_name') or not production_info.get('namespace'):\n",
    "        production_info[\"production_name\"] = input(\"Enter production name: \")\n",
    "        production_info[\"namespace\"] = input(\"Enter namespace: \")\n",
    "\n",
    "    # Add production to production data if it doesn't exist\n",
    "    if not production_data.production_exists(production_info['production_name']):\n",
    "        production_data.add_production(\n",
    "            production_info['production_name'],\n",
    "            production_info['namespace']\n",
    "        ) \n",
    "\n",
    "    api_crew = Crew(\n",
    "        agents=[crew.extraction_agent],\n",
    "        tasks=[crew.extract_api_specs(endpoints)],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    extracted_json = extract_json_from_markdown(api_crew.kickoff())\n",
    "    extracted_results = json.loads(extracted_json)\n",
    "    \n",
    "    if not isinstance(extracted_results, list):\n",
    "        extracted_results = [extracted_results] if extracted_results else []\n",
    "\n",
    "    final_endpoints = []\n",
    "    for endpoint_spec in extracted_results:\n",
    "        if not isinstance(endpoint_spec, dict):\n",
    "            continue\n",
    "            \n",
    "        missing_fields = [k for k, v in endpoint_spec.items() if v == 'missing']\n",
    "        if missing_fields:\n",
    "            api_crew = Crew(\n",
    "                agents=[crew.interaction_agent],\n",
    "                tasks=[crew.handle_missing_fields(missing_fields, endpoint_spec)],\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            updated_json = extract_json_from_markdown(api_crew.kickoff())\n",
    "            try:\n",
    "                updated_spec = json.loads(updated_json)\n",
    "                if isinstance(updated_spec, dict):\n",
    "                    endpoint_spec.update(updated_spec)\n",
    "            except (json.JSONDecodeError, AttributeError) as e:\n",
    "                print(f\"Failed to update endpoint spec: {e}\")\n",
    "\n",
    "        api_crew = Crew(\n",
    "            agents=[crew.validation_agent],\n",
    "            tasks=[crew.validate_api_spec(endpoint_spec)],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        validation_json = extract_json_from_markdown(api_crew.kickoff())\n",
    "        validation_result = json.loads(validation_json)\n",
    "        \n",
    "        if isinstance(validation_result, dict) and 'error' not in validation_result:\n",
    "            final_endpoints.append(endpoint_spec)\n",
    "            crew.production_data.add_endpoint(production_info['production_name'], endpoint_spec)\n",
    "        else:\n",
    "            print(f\"Endpoint validation failed: {validation_result}\")\n",
    "\n",
    "    api_crew = Crew(\n",
    "        agents=[crew.transformation_agent],\n",
    "        tasks=[crew.transform_to_openapi(final_endpoints, production_info)],\n",
    "        verbose=True\n",
    "    )\n",
    "    openapi_result = json.loads(extract_json_from_markdown(api_crew.kickoff()))\n",
    "\n",
    "    api_crew = Crew(\n",
    "        agents=[crew.reviewer_agent],\n",
    "        tasks=[crew.review_openapi_spec(openapi_result)],\n",
    "        verbose=True\n",
    "    )\n",
    "    review_json = extract_json_from_markdown(api_crew.kickoff())\n",
    "    review_result = json.loads(review_json)\n",
    "    \n",
    "    # Add validation and default values\n",
    "    if not isinstance(review_result, dict):\n",
    "        review_result = {\n",
    "            \"is_valid\": False,\n",
    "            \"approved_spec\": openapi_result,\n",
    "            \"issues\": [\"Invalid review result format\"],\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "    \n",
    "    # Ensure required keys exist\n",
    "    review_result.setdefault(\"is_valid\", False)\n",
    "    review_result.setdefault(\"approved_spec\", openapi_result)\n",
    "    review_result.setdefault(\"issues\", [])\n",
    "    review_result.setdefault(\"recommendations\", [])\n",
    "    if review_result[\"is_valid\"]:\n",
    "        \n",
    "        # TODO: Implement Iris integration\n",
    "        iris_payload = {\n",
    "            \"production_name\": production_info['production_name'],\n",
    "            \"namespace\": production_info['namespace'],\n",
    "            \"openapi_spec\": review_result[\"approved_spec\"]\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print( {\n",
    "            \"success\": False,\n",
    "            \"message\": \"OpenAPI specification not approved for integration\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"issues\": review_result.get(\"issues\", [])\n",
    "        })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in process_api_integration: {str(e)}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
